{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ramraj/.local/share/virtualenvs/sg-generation-6Q740Mlp/lib/python3.8/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import en_core_web_sm\n",
    "from nltk.data import find\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_SG_PATH = \"../VG-data/scene_graphs.json\"\n",
    "\n",
    "SRC_ANSWER_VOCAB_FILE = \"./intermediate_files/answer_vocab.txt\"\n",
    "SRC_SGQAS_OF_INTEREST_QA_DATA_FILE = \"./intermediate_files/filtered_qa_data.json\"\n",
    "\n",
    "DST_SG_FEATURES_DATA_FOLDER = \"./intermediate_files/sg_features/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_sg_data = json.load(open(SRC_SG_PATH, 'r'))\n",
    "global_qa_data = json.load(open(SRC_SGQAS_OF_INTEREST_QA_DATA_FILE, 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Dataset for GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "NO_OF_NODES = 30\n",
    "\n",
    "\n",
    "class Vertex:\n",
    "    def __init__(self, node):\n",
    "        self.id = node\n",
    "        self.adjacent = OrderedDict()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.id) + ' adjacent: ' + str([x.id for x in self.adjacent])\n",
    "\n",
    "    def add_neighbor(self, neighbor, weight=0):\n",
    "        self.adjacent[neighbor] = weight\n",
    "\n",
    "    def get_connections(self):\n",
    "        return self.adjacent\n",
    "\n",
    "    def get_id(self):\n",
    "        return self.id\n",
    "\n",
    "    def get_weight(self, neighbor):\n",
    "        return self.adjacent[neighbor]\n",
    "    \n",
    "\n",
    "def read_embedding_of_pred(image_id):\n",
    "    feature_filename = os.path.join(DST_SG_FEATURES_DATA_FOLDER, \"{}.h5\".format(image_id))\n",
    "    return h5py.File(feature_filename, 'r'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = global_sg_data[0]['relationships']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synsets': ['along.r.01'],\n",
       " 'predicate': 'ON',\n",
       " 'relationship_id': 15927,\n",
       " 'object_id': 5046,\n",
       " 'subject_id': 5045}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.vert_dict = {}\n",
    "        self.num_vertices = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.vert_dict.values())\n",
    "\n",
    "    def add_vertex(self, node):\n",
    "        self.num_vertices = self.num_vertices + 1\n",
    "        new_vertex = Vertex(node)\n",
    "        self.vert_dict[node] = new_vertex\n",
    "        return new_vertex\n",
    "\n",
    "    def get_vertex(self, n):\n",
    "        if n in self.vert_dict:\n",
    "            return self.vert_dict[n]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def add_edge(self, frm, to, cost=0):\n",
    "        # Ensure that we only the maximum number of nodes(graph vertices)\n",
    "        if frm not in self.vert_dict and (len(self.get_vertices()) < NO_OF_NODES):\n",
    "            self.add_vertex(frm)\n",
    "        if to not in self.vert_dict and (len(self.get_vertices()) < NO_OF_NODES):\n",
    "            self.add_vertex(to)\n",
    "\n",
    "        # Add the edge to the graph only if both the vertices are added\n",
    "        if frm in self.vert_dict and to in self.vert_dict:\n",
    "            self.vert_dict[frm].add_neighbor(self.vert_dict[to], cost)\n",
    "\n",
    "    def get_vertices(self):\n",
    "        return self.vert_dict.keys()\n",
    "\n",
    "    def convert_to_adj(self, image_id, is_inference=False):\n",
    "        embeddings_hf = read_sg_features(image_id)\n",
    "        \n",
    "        nodes = self.get_vertices()\n",
    "        nodes_indices = {k: v for v, k in enumerate(nodes)}\n",
    "        \n",
    "        adj_mat = [[0 for j in range(NO_OF_NODES)] for k in range(NO_OF_NODES)]     \n",
    "        adj_mat_wo_wts = [[0 for j in range(NO_OF_NODES)] for k in range(NO_OF_NODES)]\n",
    "        adj_mat_inf = [[-1 for j in range(NO_OF_NODES)] for k in range(NO_OF_NODES)]\n",
    "        for node in nodes:\n",
    "            adj_list = self.vert_dict[node].get_connections()\n",
    "\n",
    "            for adj_node in adj_list:\n",
    "\n",
    "                adj_mat[nodes_indices[self.vert_dict[node].id]][nodes_indices[adj_node.id]] = pred_embeddings[\n",
    "                    adj_list[adj_node]]\n",
    "\n",
    "                adj_mat_wo_wts[nodes_indices[self.vert_dict[node].id]][nodes_indices[adj_node.id]] = 1\n",
    "                if is_inference:\n",
    "                    adj_mat_inf[nodes_indices[self.vert_dict[node].id]][nodes_indices[adj_node.id]] = adj_list[adj_node]\n",
    "\n",
    "                # adj_mat[nodes_indices[self.vert_dict[node].id]][nodes_indices[adj_node.id]] = adj_list[adj_node]\n",
    "\n",
    "        # print(np.asarray(adj_mat).shape, \"\\t\\t : \\t\\t\", np.asarray(vis_adj_mat).shape)\n",
    "        if is_inference:\n",
    "            return adj_mat, vis_adj_mat, adj_mat_wo_wts, nodes_indices, adj_mat_inf\n",
    "        return adj_mat, vis_adj_mat, adj_mat_wo_wts, nodes_indices\n",
    "    \n",
    "\n",
    "def parse_rel_annotation(scene_graph_data):\n",
    "    \n",
    "    img_to_graph = {}\n",
    "    for img in img_ids:\n",
    "        g = Graph()\n",
    "        num = 0\n",
    "        \n",
    "        for rel in scene_graph_data['relationships']:\n",
    "            g.add_edge(rel['subject_id'], rel['object_id'], cost=1)\n",
    "            \n",
    "        img_to_graph[img] = g\n",
    "\n",
    "    return img_to_graph\n",
    "\n",
    "\n",
    "def generate_graph(scene_graph_data, is_inference=False):\n",
    "\n",
    "    adj_mats_dict = {}\n",
    "    adj_mats_wo_wts_dict = {}\n",
    "    nodes_indices_dict = {}\n",
    "    adj_mat_inf_dict = {}\n",
    "\n",
    "    img_to_SG_graph = parse_rel_annotation(scene_graph_data)\n",
    "    nodes_indices = {}\n",
    "    bias_mat_dict = {}\n",
    "    for SG_graph in img_to_SG_graph:\n",
    "        \n",
    "        adj_mat, adj_mat_wo_wts, nodes_indices = SG_graph[img].convert_to_adj()\n",
    "\n",
    "        adj_mats_dict[img] = adj_mat\n",
    "        adj_mats_wo_wts_dict[img] = adj_mat_wo_wts\n",
    "        nodes_indices_dict[img] = nodes_indices\n",
    "\n",
    "    return adj_mats_dict, adj_mats_wo_wts_dict, nodes_indices_dict\n",
    "\n",
    "\n",
    "class VQAVGSimpleDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, sg_data_path, qa_data_path, ans_vocab_data_path, feature_path):\n",
    "        self.sg_data_path = sg_data_path\n",
    "        self.qa_data_path = qa_data_path\n",
    "        self.ans_vocab_data_path = ans_vocab_data_path  \n",
    "        self.feature_path = feature_path\n",
    "        \n",
    "        self.sample_cnt = 0\n",
    "        self.data_sgvqa = []\n",
    "        self._load_dataset()\n",
    "        \n",
    "    def _load_dataset(self):\n",
    "        print('-> Loading filtered dataset ...')\n",
    "        self.ans_vocab_data = sorted(open(self.ans_vocab_data_path, 'r').read().strip().split(\"\\n\"))\n",
    "        sg_data = global_sg_data[:10]\n",
    "        qa_data = global_qa_data[:10]\n",
    "        \n",
    "        generate_graph(sg_data)\n",
    "        \n",
    "final_dataset = VQAVGSimpleDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csgraph\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    \n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAVGSimpleDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, sg_data_path, qa_data_path, ans_vocab_data_path, feature_path):\n",
    "        self.sg_data_path = sg_data_path\n",
    "        self.qa_data_path = qa_data_path\n",
    "        self.ans_vocab_data_path = ans_vocab_data_path  \n",
    "        self.feature_path = feature_path\n",
    "        \n",
    "        self.sample_cnt = 0\n",
    "        self.data_sgvqa = []\n",
    "        self._load_dataset()\n",
    "        \n",
    "    def _load_dataset(self):\n",
    "        print('-> Loading filtered dataset ...')\n",
    "        self.ans_vocab_data = sorted(open(self.ans_vocab_data_path, 'r').read().strip().split(\"\\n\"))\n",
    "        sg_data = global_sg_data[:10]\n",
    "        qa_data = global_qa_data[:10]\n",
    "#         sg_data = json.load(open(self.sg_data_path, 'r'))\n",
    "#         qa_data = json.load(open(self.qa_data_path, 'r'))            \n",
    "        \n",
    "        for sample_img, sample_ans in zip(sg_data, qa_data):\n",
    "            if sample_img['image_id'] != sample_ans['id']:\n",
    "                print(\"IDs did not match !\")\n",
    "                continue                \n",
    "                \n",
    "            feature_filename = os.path.join(DST_SG_FEATURES_DATA_FOLDER, \"{}.h5\".format(sample_img['image_id']))\n",
    "            with h5py.File(feature_filename, 'r') as hf:\n",
    "                \n",
    "                g = nx.Graph()\n",
    "                feature_matrix = []\n",
    "                for obj in sample_img['objects']:\n",
    "                    obj_name = obj['names'][0]\n",
    "                    obj_id = obj['object_id']                    \n",
    "                    emb_vec = np.array(hf.get(str(obj_id)))\n",
    "\n",
    "                    g.add_node(obj_id, feature=emb_vec)\n",
    "                    feature_matrix.append(emb_vec)\n",
    "                        \n",
    "                for rel in sample_img['relationships']:\n",
    "                    g.add_edge(rel['subject_id'], rel['object_id'], id=rel['relationship_id'])\n",
    "\n",
    "                adj = nx.adjacency_matrix(g)\n",
    "                # print(adj.todense())\n",
    "                adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "                sparse_mx = adj.tocoo().astype(np.float32)\n",
    "                # adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "                adj = np.array(adj.todense())        \n",
    "                \n",
    "                # feature_matrix = normalize(np.asarray(feature_matrix))\n",
    "                # feature_matrix = torch.FloatTensor(np.array(feature_matrix.todense()))\n",
    "#                 print(np.array(feature_matrix, np.float32).shape)\n",
    "#                 feature_matrix = torch.FloatTensor(np.array(feature_matrix, np.float32))\n",
    "                \n",
    "                \n",
    "                adj_matrix_per_sample = []    \n",
    "                feature_matrix_per_sample = []\n",
    "                for qa_index, qa in enumerate(sample_ans['qas']):\n",
    "                    if (qa['qas_skip']): continue\n",
    "                    question = qa['question']\n",
    "                    answer = qa['answer'].replace(\".\", \"\").lower()\n",
    "                    # sg = sample_img  \n",
    "                    \n",
    "                    adj_matrix_per_sample.append(adj)\n",
    "                    feature_matrix_per_sample.append(feature_matrix)\n",
    "                \n",
    "                print(len(feature_matrix_per_sample))\n",
    "                print(len(adj_matrix_per_sample))\n",
    "                print(len(feature_matrix_per_sample[0]))\n",
    "                print(len(adj_matrix_per_sample[0]))\n",
    "                print(adj_matrix_per_sample[0].shape)\n",
    "                f = np.array(feature_matrix_per_sample, np.float32)\n",
    "                a = np.array(adj_matrix_per_sample)\n",
    "                print(f.shape)\n",
    "                print(a.shape)\n",
    "                feature_matrix_per_sample = torch.FloatTensor(np.array(feature_matrix_per_sample, np.float32))\n",
    "                adj_matrix_per_sample = torch.FloatTensor(np.array(adj_matrix_per_sample))\n",
    "                \n",
    "                self.data_sgvqa.append({\"question\": question, \"answer\": answer,\n",
    "                                        \"sg_adj\": adj_matrix_per_sample, \"sg_feat\": feature_matrix})\n",
    "                \n",
    "                self.sample_cnt += 1\n",
    "                if (self.sample_cnt > 10): break # todo: remove                       \n",
    "        \n",
    "        print('-> Finished loading data : num. samples -> {}'.format(self.sample_cnt))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.sample_cnt\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        if index < self.sample_cnt:\n",
    "            item = self.data_sgvqa[index]\n",
    "        else:\n",
    "            item = self.data_sgvqa[index - self.sample_cnt]\n",
    "        return item\n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(self.ans_vocab_data)\n",
    "\n",
    "#     def vocab_words(self):\n",
    "#         return self.dataset_vqa.vocab_words()\n",
    "\n",
    "#     def vocab_answers(self):\n",
    "#         return self.dataset_vqa.vocab_answers()\n",
    "\n",
    "    def data_loader(self, batch_size=10, num_workers=4, shuffle=False):\n",
    "        return DataLoader(self, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=num_workers, \n",
    "                          pin_memory=True)\n",
    "\n",
    "    def split_name(self, testdev=False):\n",
    "        return self.data_sgvqa.split_name(testdev=testdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Loading filtered dataset ...\n",
      "61\n",
      "61\n",
      "40\n",
      "40\n",
      "(40, 40)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-d8d507c6021a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m vqa_dataloader = VQAVGSimpleDataset(SRC_SG_PATH, SRC_SGQAS_OF_INTEREST_QA_DATA_FILE, SRC_ANSWER_VOCAB_FILE,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                     DST_SG_FEATURES_DATA_FOLDER)\n",
      "\u001b[0;32m<ipython-input-93-b08530c68e70>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sg_data_path, qa_data_path, ans_vocab_data_path, feature_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_cnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_sgvqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-b08530c68e70>\u001b[0m in \u001b[0;36m_load_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_matrix_per_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_matrix_per_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_matrix_per_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_matrix_per_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "vqa_dataloader = VQAVGSimpleDataset(SRC_SG_PATH, SRC_SGQAS_OF_INTEREST_QA_DATA_FILE, SRC_ANSWER_VOCAB_FILE,\n",
    "                                    DST_SG_FEATURES_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
