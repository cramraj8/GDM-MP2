{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Graph data & Q data with answer labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_SG_PATH = \"../VG-data/scene_graphs.json\"\n",
    "SRC_QA_FILE = \"../VG-data/question_answers.json\"\n",
    "\n",
    "DST_ANSWER_VOCAB_FILE = \"./intermediate_files/answer_vocab.txt\"\n",
    "# DST_SGQAS_OF_INTEREST_SG_DATA_FILE = \"./intermediate_files/filtered_sg_data.json\"\n",
    "DST_SGQAS_OF_INTEREST_QA_DATA_FILE = \"./intermediate_files/filtered_qa_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108077\n",
      "108077\n"
     ]
    }
   ],
   "source": [
    "sg_data = json.load(open(SRC_SG_PATH, 'r'))\n",
    "print(len(sg_data)) # num-images\n",
    "\n",
    "qa_data = json.load(open(SRC_QA_FILE, 'r'))\n",
    "print(len(qa_data)) # num-qa (== num-img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['relationships', 'image_id', 'objects'])\n",
      "dict_keys(['id', 'qas'])\n"
     ]
    }
   ],
   "source": [
    "sg_image = sg_data[0]\n",
    "print(sg_image.keys())\n",
    "\n",
    "qa_for_image = qa_data[0]\n",
    "print(qa_for_image.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Ground Truth Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of answers :  749815\n",
      "len of set of answers :  12810\n"
     ]
    }
   ],
   "source": [
    "# Let's do QA on single word QAs\n",
    "\n",
    "qa_answers = []\n",
    "for qas_index, qas in enumerate(qa_data):\n",
    "    for qa in qas['qas']:\n",
    "        answer = qa['answer'].replace(\".\", \"\") # preprocessing-1\n",
    "        if len(answer.split(\" \")) > 1: # multiple answers\n",
    "            continue\n",
    "        elif len(answer.split(\",\")) > 1: # multi-seq answers\n",
    "            continue\n",
    "        elif not answer.isalnum():\n",
    "            continue\n",
    "        \n",
    "#         if answer.isdigit():\n",
    "#             print(answer)\n",
    "#         multi_answers = answer.split(\",\")\n",
    "#         if len(multi_answers) > 1:\n",
    "#             qa_answers.extend(multi_answers)\n",
    "        else:\n",
    "            try:\n",
    "                qa_answer_int = int(answer) # '3', '456'\n",
    "                if len(answer) == 1: # '3'\n",
    "                    qa_answers.append(answer)\n",
    "#                 else:\n",
    "#                     print(answer)\n",
    "            except: # 'clock'                \n",
    "                qa_answers.append(answer.lower())\n",
    "\n",
    "    if (qas_index == 1000000): break\n",
    "\n",
    "print(\"len of answers : \", len(qa_answers))\n",
    "qa_answers_set = list(set(qa_answers))\n",
    "# for qa_answer in qa_answers: print(qa_answer)\n",
    "print(\"len of set of answers : \", len(qa_answers_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_vocab_dict = collections.Counter(qa_answers) # elements, get, items, keys, values, most_common\n",
    "len(set(answer_vocab_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So can take all unique single answer tokens as classification class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DST_ANSWER_VOCAB_FILE, 'w') as f:\n",
    "    for answer_vocab in answer_vocab_dict.keys():\n",
    "        f.write(\"{}\\n\".format(answer_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Filtered Dataset File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed samples of : 10000 / 108077\n",
      "processed samples of : 20000 / 108077\n",
      "processed samples of : 30000 / 108077\n",
      "processed samples of : 40000 / 108077\n",
      "processed samples of : 50000 / 108077\n",
      "processed samples of : 60000 / 108077\n",
      "processed samples of : 70000 / 108077\n",
      "processed samples of : 80000 / 108077\n",
      "processed samples of : 90000 / 108077\n",
      "processed samples of : 100000 / 108077\n"
     ]
    }
   ],
   "source": [
    "# filtered_data = collections.defaultdict(list)\n",
    "n_data = len(sg_data)\n",
    "sample_cnt = 0\n",
    "for sample_img, sample_ans in zip(sg_data, qa_data):\n",
    "    if sample_img['image_id'] != sample_ans['id']:\n",
    "        print(\"IDs did not match !\")\n",
    "        continue\n",
    "    \n",
    "    # no constraint on SG\n",
    "    # sample_sg_data = copy.deepcopy(sample_img)\n",
    "    # del sample_sg_data['image_id']\n",
    "    # no constraint on Question\n",
    "    # answer constraint\n",
    "    for qa_index, qa in enumerate(sample_ans['qas']):\n",
    "            question = qa['question']\n",
    "        \n",
    "            answer = qa['answer'].replace(\".\", \"\") # preprocessing-1\n",
    "            if len(answer.split(\" \")) > 1: # multiple answers\n",
    "                qa['qas_skip'] = 1\n",
    "                continue\n",
    "            elif len(answer.split(\",\")) > 1: # multi-seq answers\n",
    "                qa['qas_skip'] = 1\n",
    "                continue\n",
    "            elif not answer.isalnum():\n",
    "                qa['qas_skip'] = 1\n",
    "                continue\n",
    "            else:\n",
    "#                 sample_composed_data = {\"qas_id\": qa['qa_id'],\n",
    "#                                         \"question\": question, \n",
    "#                                         \"answer\": answer,\n",
    "#                                         \"sg_objects\": sample_sg_data['objects'], \"sg_relationships\": sample_sg_data['relationships']}\n",
    "                try:\n",
    "                    qa_answer_int = int(answer) # '3', '456'\n",
    "                    if len(answer) == 1: # '3'\n",
    "                        # filtered_data[sample_img['image_id']].append(sample_composed_data)\n",
    "                        qa['qas_skip'] = 0\n",
    "                    else:\n",
    "                        qa['qas_skip'] = 1\n",
    "                except: # 'clock'                \n",
    "                    # filtered_data[sample_img['image_id']].append(sample_composed_data)\n",
    "                    qa['qas_skip'] = 0\n",
    "    sample_cnt += 1\n",
    "    if (sample_cnt % 10000 == 0):\n",
    "        print(\"processed samples of : {} / {}\".format(sample_cnt, n_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DST_SGQAS_OF_INTEREST_QA_DATA_FILE, 'w') as f_qa:\n",
    "    json.dump(qa_data, f_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['relationships', 'image_id', 'objects'])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_image = sg_data[0]\n",
    "sg_image.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synsets': ['along.r.01'],\n",
       " 'predicate': 'ON',\n",
       " 'relationship_id': 15927,\n",
       " 'object_id': 5046,\n",
       " 'subject_id': 5045}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_image['relationships'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synsets': ['clock.n.01'],\n",
       " 'h': 339,\n",
       " 'object_id': 1058498,\n",
       " 'names': ['clock'],\n",
       " 'w': 79,\n",
       " 'attributes': ['green', 'tall'],\n",
       " 'y': 91,\n",
       " 'x': 421}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_image['objects'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import en_core_web_sm\n",
    "from nltk.data import find\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)\n",
    "lmtzr = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "DST_SG_FEATURES_DATA_FILE = \"./intermediate_files/sg_features.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting features : 1/108077\n",
      "Finished extracting features : 101/108077\n",
      "Finished extracting features : 201/108077\n",
      "Finished extracting features : 301/108077\n",
      "Finished extracting features : 401/108077\n",
      "Finished extracting features : 501/108077\n",
      "Finished extracting features : 601/108077\n",
      "Finished extracting features : 701/108077\n",
      "Finished extracting features : 801/108077\n",
      "Finished extracting features : 901/108077\n",
      "Finished extracting features : 1001/108077\n"
     ]
    }
   ],
   "source": [
    "feature_data = []\n",
    "n_img_data = len(sg_data)\n",
    "for img_idx, sample_img in enumerate(sg_data):\n",
    "    img_id = sample_img['image_id']\n",
    "        \n",
    "    obj_features_list = []\n",
    "    for obj in sample_img['objects']:\n",
    "            obj_name = obj['names'][0]\n",
    "            obj_name_seq = nltk.word_tokenize(obj_name.lower())\n",
    "            obj_name_token_lemma = [ lmtzr.lemmatize(token) for token in obj_name_seq ]\n",
    "            emb_matrix = []\n",
    "            for token in obj_name_token_lemma:\n",
    "                try:\n",
    "                    tmp_vec = model[token]\n",
    "                    emb_matrix.append(tmp_vec)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            if emb_matrix == []:\n",
    "                emb_matrix = np.random.normal(0, 1, size=(300,))\n",
    "            emb_matrix = np.asarray(emb_matrix, np.float32)\n",
    "            semantic_feature = np.mean(emb_matrix, axis=0) # feature\n",
    "            \n",
    "            obj_features_list.append({\"object_id\": obj['object_id'], \"feature\": semantic_feature.tolist()})        \n",
    "\n",
    "    feature_data.append({\"image_id\": img_id, \"objects\": obj_features_list})\n",
    "           \n",
    "    if img_idx % 100 == 0: print(\"Finished extracting features : {}/{}\".format(img_idx + 1, n_img_data))\n",
    "    if img_idx == 1000: break\n",
    "\n",
    "\n",
    "with open(DST_SG_FEATURES_DATA_FILE, 'w') as write_file:\n",
    "    json.dump(feature_data, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying dumped json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.004914313\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sample_img, sample_feat, sample_ans in zip(sg_data, feature_data, qa_data):\n",
    "    if sample_img['image_id'] != sample_ans['id']:\n",
    "        print(\"IDs did not match !\")\n",
    "        continue\n",
    "    \n",
    "    for obj, feat in zip(sample_img['objects'], sample_feat['objects']):\n",
    "        if obj['object_id'] != feat['object_id']:\n",
    "            print(\"IDs did not match !\")\n",
    "            continue\n",
    "        feat_matrix = np.asarray(feat['feature'], np.float32)\n",
    "        print(np.mean(feat_matrix))\n",
    "        break\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Feature Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "DST_SG_FEATURES_DATA_FOLDER = \"./intermediate_files/sg_features/\"\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "if not os.path.exists(DST_SG_FEATURES_DATA_FOLDER):\n",
    "    os.makedirs(DST_SG_FEATURES_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished extracting features : 1/108077\n",
      "Finished extracting features : 101/108077\n",
      "Finished extracting features : 201/108077\n",
      "Finished extracting features : 301/108077\n",
      "Finished extracting features : 401/108077\n",
      "Finished extracting features : 501/108077\n",
      "Finished extracting features : 601/108077\n",
      "Finished extracting features : 701/108077\n",
      "Finished extracting features : 801/108077\n",
      "Finished extracting features : 901/108077\n",
      "Finished extracting features : 1001/108077\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_img_data = len(sg_data)\n",
    "for img_idx, sample_img in enumerate(sg_data):\n",
    "    img_id = sample_img['image_id']\n",
    "    \n",
    "    save_feature_file = os.path.join(DST_SG_FEATURES_DATA_FOLDER, \"{}.h5\".format(img_id))\n",
    "    with h5py.File(save_feature_file, 'w') as hf:\n",
    "        \n",
    "        obj_features_list = []\n",
    "        for obj in sample_img['objects']:\n",
    "            obj_name = obj['names'][0]\n",
    "            obj_name_seq = nltk.word_tokenize(obj_name.lower())\n",
    "            obj_name_token_lemma = [ lmtzr.lemmatize(token) for token in obj_name_seq ]\n",
    "            emb_matrix = []\n",
    "            for token in obj_name_token_lemma:\n",
    "                try:\n",
    "                    tmp_vec = model[token]\n",
    "                    emb_matrix.append(tmp_vec)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            if emb_matrix == []:\n",
    "                emb_matrix = np.random.normal(0, 1, size=(300,))\n",
    "            emb_matrix = np.asarray(emb_matrix, np.float32)\n",
    "            semantic_feature = np.mean(emb_matrix, axis=0) # feature\n",
    "\n",
    "            # save_feature_file = os.path.join(DST_SG_FEATURES_DATA_FOLDER, \"{}.h5\".format(obj['object_id']))\n",
    "            # with h5py.File(save_feature_file, 'w') as hf:\n",
    "            #     hf.create_dataset(\"features\", data=semantic_feature)\n",
    "            # hf.create_dataset(str(obj['object_id']), data=semantic_feature, chunks=True, compression=\"gzip\", compression_opts=9)\n",
    "            hf.create_dataset(str(obj['object_id']), data=semantic_feature)\n",
    "           \n",
    "    if img_idx % 100 == 0: print(\"Finished extracting features : {}/{}\".format(img_idx + 1, n_img_data))\n",
    "    if img_idx == 1000: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verfiy loading dumped features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['1058498', '1058507', '1058508', '1058511', '1058515', '1058518', '1058519', '1058525', '1058528', '1058529', '1058530', '1058531', '1058532', '1058534', '1058535', '1058536', '1058539', '1058540', '1058541', '1058542', '1058543', '1058544', '1058545', '1058546', '1058547', '1058548', '1058549', '3798575', '3798576', '3798577', '3798578', '3798579', '5045', '5046', '5048', '5049', '5050', '5051', '5055', '5060']>\n",
      "clock\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "n_img_data = len(sg_data)\n",
    "for img_idx, sample_img in enumerate(sg_data):\n",
    "    img_id = sample_img['image_id']\n",
    "    \n",
    "    feature_filename = os.path.join(DST_SG_FEATURES_DATA_FOLDER, \"{}.h5\".format(img_id))\n",
    "    with h5py.File(feature_filename, 'r') as hf:\n",
    "        print(hf.keys())\n",
    "        \n",
    "        obj_features_list = []\n",
    "        for obj in sample_img['objects']:\n",
    "            obj_name = obj['names'][0]\n",
    "            print(obj_name)\n",
    "\n",
    "            feature_vec = np.array(hf.get(str(obj['object_id'])))\n",
    "\n",
    "\n",
    "            print(feature_vec.shape)\n",
    "\n",
    "            break\n",
    "        \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
